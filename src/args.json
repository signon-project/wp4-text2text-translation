{
    "use_cuda": false,
    "seed": 222,
    "time_zone": "Europe/Madrid",

    "root_dir" : "./",
    "output_dir" : "./",
    "corpora_dir": "corpora_dgskorpus/",
    "cache_dir" : "cache/",
    "saved_models_dir" : "./",
    "last_model_dir": "model/",
    "predictions_dir": "generated_text/",
    "plots_dir": "plots/",
    "loss_history": "loss_history.npy",
    "lr_history": "lr_history.npy",
    "bleu_history": "bleu_history.npy",
    "file_offsets": "offsets.npy",
    "model_filename": "model.pt",
    "model_state_file": "model_state.pt",
    "pretrained_model": "facebook/mbart-large-cc25",
    "sentencepiece_model": "mbart.cc25.v2/sentence.bpe.model",
    "pretrained_model_vocabulary_file": "mbart.cc25.v2/dict.txt",
    "original_pruned_vocabulary_file" : "training/vocab.txt",
    "all_corpus": "training/corpus.all",
    "pruned_vocabulary_file" : "vocab.txt",
    "old_pruned_vocabulary_file": "old_vocab.txt",
    "pruned_model": "models/pruned_model_multilingualmbart/",
    "input_embeddings_file": "input_embedding_weights.pt",
    "output_embeddings_file": "output_embedding_weights.pt",
    "old_input_embeddings_file": "old_input_embedding_weights.pt",
    "old_output_embeddings_file": "old_output_embedding_weights.pt",
 
    "train_mode": "train",
    "development_mode": "development",
    "evaluation_mode": "test",
    "download_paracrawl_datasets": false,
    "vocabulary_size": 32000,
    "model_type": "last",
    "development_epochs": -1,
    "development_iterations": 100,
    "padding": true,
    "truncation": true,
    "pad_to_max_length": false,
    "dataloader_num_workers": 1,
    "dataloader_pin_memory": true,
    "dataloader_prefetch_factor": 1,
    "ignore_pad_token_for_loss": true,
    "label_pad_token_id": -100,
    "chunk_size": 30000,
    "test_size": 2000,
    "development_size": 500,
    "drop_last": false,
    "batch_bucketing": false,
    "bucket_size_multiplier": 10000,
    "non_blocking": false,
    "always_save_metrics": true,
    "metrics": ["SacreBLEU"],
    "development_metric": "SacreBLEU",
    "no_repeat_ngram_size": 3,
    "repetition_penalty": 1.2,

    "pruned_vocabulary": true,
    "use_fp16": true,

    "use_cross_entropy_smoothing": false,
    "cross_entropy_smoothing": 0.2,
    "learning_rate": 3e-5,
    "per_device_train_batch_size" : 1,
    "per_device_eval_batch_size": 10,
    "gradient_accumulation_steps": 4,
    "num_train_epochs": -1,
    "train_iterations": 1000,
    "train_updates": -1,
    "adam_eps": 1e-6,
    "adam_betas": [0.9, 0.999],
    "weight_decay": 1e-3,
    "lr_scheduler_type": "polynomial",
    "num_warmup_steps": 0,
    "save_models_each_n_iters": 100,
    "resume_training": false,

    "tokeniser_dir": "tokeniser/",
    "old_tokeniser_dir": "old_tokeniser/",
    "tokeniser_prefix": "interl",
    "tokeniser_character_coverage": 1.0,
    "tokeniser_input_sentence_size": 100000,
    "max_samples_per_lang": 100000,
    "unk_token": "<unk>",
    "bos_token": "<s>",
    "eos_token": "</s>",
    "pad_token": "<pad>",

    "encoder_layers": 3,
    "decoder_layers": 3,
    "encoder_attention_heads": 4,
    "decoder_attention_heads": 4,
    "encoder_ffn_dim": 512,
    "decoder_ffn_dim": 512,
    "embedding_length": 1024,
    "attention_dropout": 0.1,
    "classifier_dropout": 0.3,
    "num_beams": 5,
    "train_max_length": 400,
    "val_max_length": 512,
    "freeze_embeddings": false,
    
    "paracrawl_languages": ["en-ga", "en-nl", "en-es"],
    "languages": ["en-ga", "ga-en", "es-en", "en-nl", "en-es", "nl-en"],
    "language_codes" : [
        "ar_AR",
        "cs_CZ",
        "de_DE",
        "en_XX",
        "es_XX",
        "et_EE",
        "fi_FI",
        "fr_XX",
        "gu_IN",
        "hi_IN",
        "it_IT",
        "ja_XX",
        "kk_KZ",
        "ko_KR",
        "lt_LT",
        "lv_LV",
        "my_MM",
        "ne_NP",
        "nl_XX",
        "ro_RO",
        "ru_RU",
        "si_LK",
        "tr_TR",
        "vi_VN",
        "zh_CN"
    ]
}
